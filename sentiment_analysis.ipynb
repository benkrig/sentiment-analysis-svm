{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis para clasificar críticas de películas\n",
    "\n",
    "## Introducción\n",
    "La idea de este notebook es establecer un método general para aplicar _sentiment analysis_ adaptable a otros datasets.\n",
    "\n",
    "Primero vamos a ver las diferentes features que podemos extraer de un texto relativas a _sentiment analysis_. Construiremos un extractor de features generalizado que pueda valer para la mayoría de problemas\n",
    "\n",
    "Luego aplicaremos las features a diferentes modelos, como _Support Vector Machines_ (__SVM__) o un clasificador _Naive Bayes_.\n",
    "\n",
    "Lo interesante de esta estrategia es que nos permitirá saber automáticamente cuáles son las mejores features para nuestro problema mediante una búsqueda de parámetros.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación\n",
    "\n",
    "Las bibliotecas de python usadas para este proyecto son `nltk` y `scikit-learn`. Antes de empezar hay que descargar los corpus necesarios para las diferentes utilidades de `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to /home/marhs/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('tagsets')\n",
    "# TODO Faltan aqui un par de downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "\n",
    "_Sentiment analysis_ es la extracción/análisis de opiniones subjetivas (no objetivas) sobre un texto. Puede ser la polarización del texto (si habla bien o mal de algún tema), clasificación de sentimientos: si el emisor del mensaje está contento, enfadado, triste, etc, diferentes actitudes hacia un tema, como agresividad o calma.\n",
    "\n",
    "El uso de este tipo de técnicas en la vida real es muy amplio, ya que permite automatizar procesos de naturaleza ambigua como:\n",
    "\n",
    "* Medidor de opiniones sobre una tema en particular en redes sociales (twitter, facebook)\n",
    "* Identificación de contenido inapropiado (insultos, _flames_) para automoderación de comunidades\n",
    "* Detección de sesgos en medios de comunicación como periódicos\n",
    "\n",
    "En este caso vamos a clasificar críticas de películas hechas por usuarios (en inglés). La idea es que si tenemos críticas escritas en foros, twitter, etc podremos clasificar si una película es buena o mala en base al la reacción del público en general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis como un problema de clasificación\n",
    "\n",
    "Para simplificar el problema y poder aplicar técnicas ya conocidas de clasificación vamos a crear un modelo que clasifique las críticas de usuarios en dos categorías: buenas (1) y malas (0). Como en los problemas de clasificación en general, la idea es convertir cada documento en una serie de features y entrenar un modelo que clasifique en nuestras dos categorías (0 y 1) dadas una features.\n",
    "\n",
    "### Vector space model\n",
    "\n",
    "El modelo elegido para este caso es el _vector space model_ o _term vector model_. Es este modelo consideramos que un documento es una colección de palabras. Cada documento puede ser clasificado en base a la __frecuencia de palabras__ que aparecen en él. Una mala crítica dejará una frecuencia más alta de palabras malas como: _horrible, worst, bad, worthless_, mientras que una buena dejará una frecuencia alta de buenas palabras: _great, enjoyable, best_.\n",
    "\n",
    "Hay determinadas palabras, como _\"of\"_ o _\"the\"_ que van a aparecer con una alta frecuencia en la mayoría de documentos. Para que esto no haga más difícil clasificar documentos en base a la frecuencia poodemos probar a añadir también la __frecuencia inversa__: como de frecuente es la palabra en cuestión entre todos los documentos disponibles.\n",
    "\n",
    "Este modelo se conoce como __TF-IDF__: _Term frequency-inverse document frequency_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracción de features\n",
    "\n",
    "Aunque hemos hablado de frecuencia de palabras en los _vector space model_ podemos jugar mucho con la definición de \"palabra\" para generar diferentes modelos.\n",
    "\n",
    "En procesamiento de lenguaje natural hemos visto una aproximación interesante al análisis de textos: los _n-gramas_: secuencias de n elementos del texto a analizar que nos permiten analizar por separado estructuras del texto, ya sean palabras (unigramas) o cadenas más largas de palabras (bigramas, trigramas).\n",
    "\n",
    "Podemos generar diferentes modelos de _term frequency_ usando unigramas, bigramas, etc.\n",
    "\n",
    "Vamos a cargar un ejemplo del conjunto de entrenamiento para ver como extraer diferentes características. Para este ejemplo vamos a cargar la review negativa \\#3886 del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is this a stupid movie? You bet!! I could not find any moment in this film that was creepy or scary. Stupid moments? Plenty. Stupid characters? You bet. Bad effects? Everywhere! Rick Baker may have gone and done bigger and better things, this is not one of them. Oh well people gotta start somewhere. Dr. Ted Nelson is cheesed. He is the most whiny doctor I've ever seen. He's got a melting man running amok out in Ventura County somewhere, he's not overly happy that his wife is pregnant (probably cause she's 55 years old and weighs 90 lbs) and there's no crackers to be found anywhere. Plus he's got the not-too-helpful general on his hinder wanting to find astronaut Steve. And the local sheriff wants to know what's going on even though Mr. Nelson can't tell him anything. There also some random characters thrown in for good measure who encounter the melting man. Eventually the movie ends and out monster gets scooped into a trash can to become compost. In the end it's just what you need for a great MST episode.\n"
     ]
    }
   ],
   "source": [
    "raw_text = open('data/train/neg/3886_1.txt').read()\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigramas y bigramas\n",
    "\n",
    "Vamos a usar `nltk` como herramienta de __NLP__. `nltk` posee la utilidad `word_tokenize`, que dado un texto lo divide en palabras, generando así su unigrama. Con `nltk.util.ngrams` y este unigrama podemos generar n-gramas de la longitud deseada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lbs',\n",
       " ')',\n",
       " 'and',\n",
       " 'there',\n",
       " \"'s\",\n",
       " 'no',\n",
       " 'crackers',\n",
       " 'to',\n",
       " 'be',\n",
       " 'found',\n",
       " 'anywhere',\n",
       " '.',\n",
       " 'Plus',\n",
       " 'he',\n",
       " \"'s\",\n",
       " 'got',\n",
       " 'the',\n",
       " 'not-too-helpful',\n",
       " 'general',\n",
       " 'on']"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(raw_text)\n",
    "tokens[120:140]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'could'),\n",
       " ('could', 'not'),\n",
       " ('not', 'find'),\n",
       " ('find', 'any'),\n",
       " ('any', 'moment'),\n",
       " ('moment', 'in'),\n",
       " ('in', 'this'),\n",
       " ('this', 'film'),\n",
       " ('film', 'that'),\n",
       " ('that', 'was')]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram = list(ngrams(tokens, n=2))\n",
    "bigram[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver que `word_tokenize` ha separado todas las palabras del texto, con algunos errores. Conceptos como _not-too-helpful_ han sido imposibles de separar ya que ha interpretado los guiones como una misma palabra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Es posible que nos encontremos palabras con significados muy parecidos a la hora de clasificar en _sentiment analysis_ pero que al ser palabras distintas vayan a contar por separado. Un ejemplo de esto podría ser __*happy*__ (feliz) junto a __*happier*__ (más feliz que). Ambos ejemplos aportan la misma idea: felicidad, pero si solo usamos la frecuencia de palabras van a ser analizadas por separado.\n",
    "\n",
    "Una posible aproximación para arreglar esto es usar solo la raíces _(stems)_ de las palabras. De esta manera _happy_ y _happier_ se usarían como __*happi*__, aumentando la frecuencia del mismo concepto. También van a ayudar a transformarlo todo a minúsculas.\n",
    "\n",
    "Los diferentes _stemmers_ disponibles en NLTK no son perfectos, pero podemos ver como funcionan sobre un conjunto de palabras que ya hemos sacado en el ejemplo anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "snow_stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos cada palabra con su raíz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('was', 'was'),\n",
       " ('creepy', 'creepi'),\n",
       " ('or', 'or'),\n",
       " ('scary', 'scari'),\n",
       " ('.', '.'),\n",
       " ('Stupid', 'stupid'),\n",
       " ('moments', 'moment'),\n",
       " ('?', '?'),\n",
       " ('Plenty', 'plenti'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stems = [(word, snow_stemmer.stem(word)) for word in tokens]\n",
    "stems[20:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of speech (POS) tagging\n",
    "\n",
    "Por último podemos aprovechar toda la potencia que da el análisis POS. _Part of speech_ (POS) es el análisis sintáctico del texto. Nos va a permitir saber cual es el uso de cada una de las palabras(verbo, nombre, adjetivos). De esta manera podremos afinar mucho más en aquellos sitios donde sabemos que está nuestra información.\n",
    "\n",
    "En este caso, sabemos que las opiniones se producen sobre todo en adjetivos y adverbios (bueno, malo, peor que, etc), por lo tanto podemos hacer un filtrado por este tipo de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('could', 'MD'),\n",
       " ('not', 'RB'),\n",
       " ('find', 'VB'),\n",
       " ('any', 'DT'),\n",
       " ('moment', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('film', 'NN'),\n",
       " ('that', 'WDT')]"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_tokens = pos_tag(tokens)\n",
    "tagged_tokens[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK identifica cada palabra con una etiqueta. En su ayuda muestra el significado de cada una de las etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En nuestro caso queremos quedarnos con adjetivos, cuyas etiquetas empiezan por __JJ\\*__ y los adverbios, que empiezan por __RB\\*__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('stupid', 'JJ'),\n",
       " ('not', 'RB'),\n",
       " ('scary', 'JJ'),\n",
       " ('Stupid', 'JJ'),\n",
       " ('bet', 'RB'),\n",
       " ('Everywhere', 'RB'),\n",
       " ('bigger', 'JJR'),\n",
       " ('better', 'JJR'),\n",
       " ('not', 'RB'),\n",
       " ('well', 'RB')]"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token, tag) for token, tag in tagged_tokens\n",
    " if tag[:2] == 'JJ' or tag[:2] == 'RB'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composición de pasos\n",
    "\n",
    "Tenemos varias ideas para probar con nuestro _vector space model_. Vamos a crear una función que, dependiendo de diferentes parámetros, convierta un texto a la forma deseada. De esta manera vamos a facilitar mucho la experimentación de pruebas a posteriori.\n",
    "\n",
    "Si en un futuro queremos experimentar con la forma de sacar términos, solo habría que cambiar la siguiente función, dejando el resto del flujo sin modificar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(text, use_stem=True, stemmer=snow_stemmer, use_pos=False, \n",
    "                     use_only_adj=False, use_bigrams=False, use_bigrams_only=False):\n",
    "    # Separate words\n",
    "    words = word_tokenize(text)\n",
    "    # PoS tagging words\n",
    "    if use_pos:\n",
    "        pos_tags = nltk.pos_tag(words)\n",
    "    else:\n",
    "        pos_tags = zip(words, [''] * len(words))\n",
    "    \n",
    "    tokens = []\n",
    "    # Special treatment for bigrams\n",
    "    if use_bigrams:\n",
    "        tokens += list(ngrams(words, n=2))\n",
    "        if use_bigrams_only:\n",
    "            return tokens\n",
    "        else:\n",
    "            tokens += [(x,) for x in words]\n",
    "        return tokens\n",
    "    \n",
    "    for word, tag in pos_tags:\n",
    "        res_word = word\n",
    "        use_word = True\n",
    "        # Convert to stem\n",
    "        if use_stem:\n",
    "            res_word = stemmer.stem(res_word)\n",
    "        # Use POS tag with the word\n",
    "        if use_pos and not use_only_adj:\n",
    "            res_word += '_' + tag\n",
    "        # Only use adv and adj\n",
    "        if use_only_adj and not (tag[:2] == 'JJ' or tag[:2] == 'RB'):\n",
    "            use_word = False\n",
    "        # Append the word to the tokenizer\n",
    "        if use_word:\n",
    "            tokens.append(res_word)\n",
    "    return tokens\n",
    "\n",
    "def text_stems_tok(text):\n",
    "    return custom_tokenizerC\n",
    "def pos_tok(text):\n",
    "    return custom_tokenizer(text, use_stem=False, use_pos=True)\n",
    "def pos_stems_tok(text):\n",
    "    return custom_tokenizer(text, use_stem=True, use_pos=True)\n",
    "def adj_tok(text):\n",
    "    return custom_tokenizer(text, use_stem=False, use_pos=True, use_only_adj=True)\n",
    "def adj_stems_tok(text):\n",
    "    return custom_tokenizer(text, use_stem=True, use_pos=True, use_only_adj=True)\n",
    "def unigrams(text):\n",
    "    return word_tokenize(text)\n",
    "def uni_bigrams(text):\n",
    "    return custom_tokenizer(text, use_bigrams=True)\n",
    "def bigrams(text):\n",
    "    return custom_tokenizer(text, use_bigrams=True, use_bigrams_only=True)\n",
    "def uni_bigrams_stems(text):\n",
    "    tokens = uni_bigrams(text)\n",
    "    res_tokens = []\n",
    "    for t in tokens:\n",
    "        res_tokens.append(tuple([snow_stemmer.stem(x) for x in t]))\n",
    "    return res_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stupid',\n",
       " 'not',\n",
       " 'scary',\n",
       " 'Stupid',\n",
       " 'bet',\n",
       " 'Everywhere',\n",
       " 'bigger',\n",
       " 'better',\n",
       " 'not',\n",
       " 'well']"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = adj_tok(raw_text)\n",
    "features[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación del _vector space model_\n",
    "\n",
    "Para la creación y entrenamiento del modelo vamos a usar `scikit-learn`. Scikit-learn ofrece utilidades para la carga de un dataset, creaciones de diferentes tipos de modelos y entrenamientos. Primero cargamos el dataset de entrenamiento y el de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_files('data/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_files('data/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que ha generado una clase numérica por cada una de las clases encontradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 neg\n",
      "1 pos\n"
     ]
    }
   ],
   "source": [
    "for t in range(len(dataset.target_names)):\n",
    "    print(t, dataset.target_names[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer\n",
    "\n",
    "La utilidad `CountVectorizer` nos va a permitir transformar nuestros textos a un array de frecuencias de cada uno de los términos de la colección de documentos. Permite incorporar una función personalizada para transformar estos textos en términos y es aquí donde vamos a incorporar nuestras funciones previamente definidas.\n",
    "\n",
    "Va a convertir cada uno de los textos en una matriz dispersa (_sparse_) donde las columnas representarán los diferentes términos y las columnas los documentos en los que aparecen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 114666)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer(tokenizer=unigrams)\n",
    "X_train_counts = count_vect.fit_transform(dataset.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Transformer\n",
    "\n",
    "Una vez tenemos el conteo de términos por documento es necesario averiguar la frecuencia de estos. Como comentamos antes, no solo vamos a usar la frecuencia (__TF__) sino también la frecuencia inversa (__IDF__). La clase `TfidfTransformer` permite hacer todo esto en un solo paso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 114666)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=True).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines como clasificador\n",
    "\n",
    "Ya tenemos matrices que indican la frecuencia de cada termino menos la frecuencia inversa. Ahora necesitamos entrenar un clasificador que nos permita transformar esta matriz a una de las dos categorías que tenemos: pos o neg.\n",
    "\n",
    "Aquí podríamos probar varios clasificadores, pero me he centrado en el que mejor me suele funcionar para analisis de frecuencia: __Support Vector Machines SVM__. Una _máquina de soporte vectorial_ es un modelo que representa los puntos de muestra en el espacio, separando las clases a dos espacios lo más amplios posibles mediante un hiperplano.\n",
    "\n",
    "Es fácil de visualizar cuando los puntos de muestra tienen dos dimensiones, como en la siguiente imagen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"svm.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos el clasificador por SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ajustamos el clasificador a nuestro conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,\n",
       "       penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
       "       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf = SGDClassifier()\n",
    "text_clf.fit(X_train_tf, dataset.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto, el modelo está listo para clasificar. Ahora podemos probarlo con algún ejemplo del conjunto de test, pero primero tenemos que transformar el conjunto de test a nuestras features __TF-IDF__.\n",
    "\n",
    "### SKLearn Pipelines\n",
    "\n",
    "Cuando tenemos un proceso definido mediantes clases de transformación de sklearn, en vez de tener que recrearlo entero cada vez que queramos trabajar con estos datos podemos definir un _pipeline_. Con este _pipeline_ podremos trabajar como si fuera un modelo único que genera todas las transformaciones y luego clasifica.\n",
    "\n",
    "Lo interesante de usar este proceso es que al usar input/outputs estandarizados por sklearn, solo haría falta cambiar la línea de \n",
    "```\n",
    "('clf', SGDClassifier())\n",
    "```\n",
    "por\n",
    "```\n",
    "('clf', MultinomialNB())\n",
    "```\n",
    "para usar un clasficador Naive Bayes en vez de una SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(tokenizer=uni_bigrams_stems)),\n",
    "                     ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                     ('clf', SGDClassifier())\n",
    "                    ])\n",
    "                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajustamos el modelo a los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = text_clf.fit(dataset.data, dataset.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos como se comporta el modelo frente al conjunto de test. Para ello, predecimos todos los documentos del conjunto de test y vemos con cuantos hemos acertado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89651999999999998"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "predicted = text_clf.predict(test_dataset.data)\n",
    "np.mean(predicted == test_dataset.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto significa que el modelo usando solo bigramas tiene casi un 90% de acierto en el conjunto de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elección de modelos y cross-validation\n",
    "\n",
    "Una de las ventajas de usar sklearn es que en vez de probar todas las diferentes funciones que definimos al principio podemos ejecutar una búsqueda para quedarnos con la que tenga mejor puntuación. Para que no triunfe el modelo que mejor se sobreajuste al conjunto de test es interesante hacer cross validación con el conjunto de entrenamiento.\n",
    "\n",
    "## Cross-validation\n",
    "\n",
    "La cross validación consiste en dividir el conjunto de entrenamiento en n grupos iguales (en este caso usamos 3 grupos), entrenar el modelo con n-1 grupos y comprobar la eficiencia de este en el grupo restante. Esto se hace n veces y al final podemos saber la puntuación media sobre los n grupos, evitando un posible sobreajuste.\n",
    "\n",
    "## Búsqueda de parámetros\n",
    "\n",
    "Para la búsqueda de parámetros y la cross validación usamos la clase GridSearchCV, que nos permite definir un modelo y n parámetros. Entrenará el modelo para todos las combinaciones posibles de esos n parámetros y devolverá el modelo con mejor puntuación.\n",
    "\n",
    "Teniendo en cuenta que no tenemos un dataset pequeño y que los análisis POS no son rápidos, una búsqueda de parámetros puede ser muy lenta (en mi caso la siguiente búsqueda ha tardado aproximadamente 8 horas).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88207999999999998"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', SGDClassifier()),\n",
    "                        ])\n",
    "\n",
    "parameters = {\n",
    "    'vect__tokenizer': (\n",
    "        word_tokenize, text_stems_tok, pos_tok,\n",
    "        pos_stems_tok, adj_tok, adj_stems_tok\n",
    "    ),\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'clf__alpha': (1e-3, 1e-4),\n",
    "}\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf_svm, parameters, n_jobs=-1)\n",
    "gs_clf.fit(dataset.data, dataset.target)\n",
    "predicted = gs_clf.predict(test_dataset.data)\n",
    "np.mean(predicted == test_dataset.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si importamos los resultados de la búsqueda podemos ver todos los posibles entrenamientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_clf__alpha</th>\n",
       "      <th>param_tfidf__use_idf</th>\n",
       "      <th>param_vect__tokenizer</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42.809086</td>\n",
       "      <td>20.595768</td>\n",
       "      <td>0.83835</td>\n",
       "      <td>0.861900</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>&lt;function word_tokenize at 0x7fd443731e18&gt;</td>\n",
       "      <td>{'clf__alpha': 0.001, 'vect__tokenizer': &lt;func...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.835158</td>\n",
       "      <td>0.861397</td>\n",
       "      <td>0.835608</td>\n",
       "      <td>0.861697</td>\n",
       "      <td>0.844284</td>\n",
       "      <td>0.862607</td>\n",
       "      <td>0.676449</td>\n",
       "      <td>0.193819</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.000515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92.014162</td>\n",
       "      <td>44.660706</td>\n",
       "      <td>0.84560</td>\n",
       "      <td>0.864775</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>&lt;function text_stems_tok at 0x7fd434280bf8&gt;</td>\n",
       "      <td>{'clf__alpha': 0.001, 'vect__tokenizer': &lt;func...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.838908</td>\n",
       "      <td>0.867622</td>\n",
       "      <td>0.846108</td>\n",
       "      <td>0.867247</td>\n",
       "      <td>0.851785</td>\n",
       "      <td>0.859457</td>\n",
       "      <td>6.561049</td>\n",
       "      <td>0.289353</td>\n",
       "      <td>0.005269</td>\n",
       "      <td>0.003764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>291.978515</td>\n",
       "      <td>156.644288</td>\n",
       "      <td>0.82990</td>\n",
       "      <td>0.853275</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>&lt;function pos_tok at 0x7fd434280c80&gt;</td>\n",
       "      <td>{'clf__alpha': 0.001, 'vect__tokenizer': &lt;func...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.832758</td>\n",
       "      <td>0.859446</td>\n",
       "      <td>0.826459</td>\n",
       "      <td>0.854571</td>\n",
       "      <td>0.830483</td>\n",
       "      <td>0.845808</td>\n",
       "      <td>8.023300</td>\n",
       "      <td>4.274308</td>\n",
       "      <td>0.002605</td>\n",
       "      <td>0.005643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>361.926162</td>\n",
       "      <td>183.052437</td>\n",
       "      <td>0.82890</td>\n",
       "      <td>0.853500</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>&lt;function pos_stems_tok at 0x7fd434280ea0&gt;</td>\n",
       "      <td>{'clf__alpha': 0.001, 'vect__tokenizer': &lt;func...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.826909</td>\n",
       "      <td>0.858171</td>\n",
       "      <td>0.828409</td>\n",
       "      <td>0.855921</td>\n",
       "      <td>0.831383</td>\n",
       "      <td>0.846408</td>\n",
       "      <td>5.289329</td>\n",
       "      <td>11.005596</td>\n",
       "      <td>0.001859</td>\n",
       "      <td>0.005099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>299.924710</td>\n",
       "      <td>149.129380</td>\n",
       "      <td>0.82605</td>\n",
       "      <td>0.848325</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>&lt;function adj_tok at 0x7fd434280d90&gt;</td>\n",
       "      <td>{'clf__alpha': 0.001, 'vect__tokenizer': &lt;func...</td>\n",
       "      <td>14</td>\n",
       "      <td>0.825109</td>\n",
       "      <td>0.848796</td>\n",
       "      <td>0.823309</td>\n",
       "      <td>0.849696</td>\n",
       "      <td>0.829733</td>\n",
       "      <td>0.846483</td>\n",
       "      <td>7.587796</td>\n",
       "      <td>4.655080</td>\n",
       "      <td>0.002706</td>\n",
       "      <td>0.001354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0      42.809086        20.595768          0.83835          0.861900   \n",
       "1      92.014162        44.660706          0.84560          0.864775   \n",
       "2     291.978515       156.644288          0.82990          0.853275   \n",
       "3     361.926162       183.052437          0.82890          0.853500   \n",
       "4     299.924710       149.129380          0.82605          0.848325   \n",
       "\n",
       "  param_clf__alpha param_tfidf__use_idf  \\\n",
       "0            0.001                 True   \n",
       "1            0.001                 True   \n",
       "2            0.001                 True   \n",
       "3            0.001                 True   \n",
       "4            0.001                 True   \n",
       "\n",
       "                         param_vect__tokenizer  \\\n",
       "0   <function word_tokenize at 0x7fd443731e18>   \n",
       "1  <function text_stems_tok at 0x7fd434280bf8>   \n",
       "2         <function pos_tok at 0x7fd434280c80>   \n",
       "3   <function pos_stems_tok at 0x7fd434280ea0>   \n",
       "4         <function adj_tok at 0x7fd434280d90>   \n",
       "\n",
       "                                              params  rank_test_score  \\\n",
       "0  {'clf__alpha': 0.001, 'vect__tokenizer': <func...                8   \n",
       "1  {'clf__alpha': 0.001, 'vect__tokenizer': <func...                5   \n",
       "2  {'clf__alpha': 0.001, 'vect__tokenizer': <func...               11   \n",
       "3  {'clf__alpha': 0.001, 'vect__tokenizer': <func...               12   \n",
       "4  {'clf__alpha': 0.001, 'vect__tokenizer': <func...               14   \n",
       "\n",
       "   split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0           0.835158            0.861397           0.835608   \n",
       "1           0.838908            0.867622           0.846108   \n",
       "2           0.832758            0.859446           0.826459   \n",
       "3           0.826909            0.858171           0.828409   \n",
       "4           0.825109            0.848796           0.823309   \n",
       "\n",
       "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "0            0.861697           0.844284            0.862607      0.676449   \n",
       "1            0.867247           0.851785            0.859457      6.561049   \n",
       "2            0.854571           0.830483            0.845808      8.023300   \n",
       "3            0.855921           0.831383            0.846408      5.289329   \n",
       "4            0.849696           0.829733            0.846483      7.587796   \n",
       "\n",
       "   std_score_time  std_test_score  std_train_score  \n",
       "0        0.193819        0.004200         0.000515  \n",
       "1        0.289353        0.005269         0.003764  \n",
       "2        4.274308        0.002605         0.005643  \n",
       "3       11.005596        0.001859         0.005099  \n",
       "4        4.655080        0.002706         0.001354  "
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(gs_clf.cv_results_)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados para algunos de los modelos son los siguientes\n",
    "\n",
    "|                      | TF-IDF  | TF      |\n",
    "|----------------------|---------|---------|\n",
    "| Unigram              | 0.88670 | 0.80215 |\n",
    "| Unigram with stems   | 0.88765 | 0.81555 |\n",
    "| Bigram               | 0.88351 | __0.85972__ |\n",
    "| Unigram + Bigram     | 0.89459 | --      |\n",
    "| Unigram + Bigram with stems    | __0.89651__ | --      |\n",
    "| POS                  | 0.88350 | 0.81745 |\n",
    "| POS with stems       | 0.88456 | 0.80490 |\n",
    "| Adj + adv            | 0.84435 | 0.83235 |\n",
    "| Adj + adv with stems | 0.84070 | 0.83200 |\n",
    "\n",
    "Como podemos ver, el mejor modelo lo ha conseguido es el que combina unigramas con bigramas de las raices de las palabras. En uno de los artículos donde usan este dataset \\[2\\] la mejor puntuación es de __0.89632__ usando unigramas + bigramas, mientras que la mía es __0.89651__ con la misma técnica pero usando las raices de los tokens.\n",
    "\n",
    "Como dato interesante, si no usamos __IDF__ nos estamos quedando sin mecanismo para eliminar palabras comunes como _the_ o signos de puntuación. No es raro entonces que en este caso los mejores modelos sean aquellos que ya incorporan algún tipo de filtro, como los bigramas (es raro que haya dos palabras de este tipo seguidas) y cuando usamos solo adjetivos y advervios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardar y cargar el modelo\n",
    "\n",
    "Como hemos visto a veces los modelos son muy lentos de entrenar, por lo que es conveniente guardarlos a disco una vez están entrenados. De esta manera, cuando haya que usarlos solo hay que cargarlos sin tener que re-entrenar nada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my_model.pkl']"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "joblib.dump(text_clf, 'my_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "loaded_clf = joblib.load('my_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliografía\n",
    "\n",
    "* \\[1\\] [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/)\n",
    "* \\[2\\] [Sentiment analysis of users' reviews and comments](http://cs229.stanford.edu/proj2012/ChakankarVenuturimilliMathur-SentimentAnalysis.pdf)\n",
    "* \\[3\\] [NLTK Book](http://www.nltk.org/book/)\n",
    "* \\[4\\] Statistics: The vector space model (J. F. Quesada)\n",
    "* \\[5\\] [Scikit-learn: Working with text data](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\n",
    "* \\[6\\] [Wikipedia: Sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
